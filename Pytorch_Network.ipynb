{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea899e0f-1e90-424f-8cad-74d8dfdfe70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary #just for debugging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5aa7d-42aa-48c4-b2f1-80fc3f2c2d09",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0d7182-0bcb-4ea0-b8d9-e1a937305adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"./Data/seg_train\"\n",
    "TEST_DIR = \"./Data/seg_test\"\n",
    "\n",
    "genre_dict = {\"buildings\":0,\"forest\":1,\"glacier\":2,\"mountain\":3,\"sea\":4,\"street\":5}\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (64,64) # resize the images\n",
    "SEED = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bccece-3257-4aa9-afdb-5ac4a2c51ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b01a1-6e4f-4c45-8092-48a30f05ac3c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57133770-430c-4508-90fd-7c20484f57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dataloader(traindir, testdir, image_size, batch_size, validation_split):\n",
    "    \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                          transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0),\n",
    "                                          transforms.RandomVerticalFlip(0.5),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          normalize])\n",
    "    \n",
    "    test_transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         normalize])\n",
    "    \n",
    "    \n",
    "    train_data = datasets.ImageFolder(traindir, transform = train_transform)\n",
    "    test_data = datasets.ImageFolder(testdir, transform = test_transform)\n",
    "    \n",
    "    \n",
    "    classes = train_data.classes\n",
    "    \n",
    "\n",
    "    length_val_ds = int(len(train_data)*validation_split)\n",
    "    length_train_ds = len(train_data) - length_val_ds\n",
    "    \n",
    "    train_data, valid_data = torch.utils.data.random_split(train_data, [length_train_ds, length_val_ds])\n",
    "    \n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size = 6)\n",
    "    validloader = torch.utils.data.DataLoader(valid_data, shuffle = True, batch_size = BATCH_SIZE)\n",
    "    testloader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size = BATCH_SIZE)\n",
    "    \n",
    "    return trainloader, validloader, testloader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5319c3-136c-427b-9101-a39bac591c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, class_names = Dataloader(traindir=TRAIN_DIR, testdir=TEST_DIR, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7626ebd6-3cdb-400f-a955-14a5e5109058",
   "metadata": {},
   "source": [
    "## Show Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885e0fb-6d44-419f-948c-332fac8760d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = iter(train_ds)\n",
    "                    \n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "counter = 0\n",
    "images = []\n",
    "                    \n",
    "while True:\n",
    "    inputs, classes = next(generator)\n",
    "    classes = classes.numpy()\n",
    "    \n",
    "    if counter in classes:\n",
    "        index = np.where(classes == counter)[0][0]\n",
    "        images.append(inputs[index])\n",
    "        counter += 1\n",
    "    \n",
    "    if counter == 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f850f8-eaa4-468c-a49f-6e56c35f4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #mean = np.array([0.485, 0.456, 0.406])\n",
    "    #std = np.array([0.229, 0.224, 0.225])\n",
    "    #inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.figure(figsize = (15,10))\n",
    "    plt.imshow(inp, aspect='auto')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.savefig('example_images.png')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(train_ds))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(images, nrow = 3)\n",
    "\n",
    "#imshow(out, title=[class_names[x] for x in classes])\n",
    "imshow(out, title=[\"buildings\",\"forest\",\"glacier\",\"mountain\",\"sea\",\"street\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2013a-e6a9-4dcf-bdb5-c7899c6bcbde",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fae9186-5dcc-4f64-8950-380dacacea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 30, 30]             896\n",
      "              ReLU-2           [-1, 32, 30, 30]               0\n",
      "       BatchNorm2d-3           [-1, 32, 30, 30]              64\n",
      "         MaxPool2d-4           [-1, 32, 15, 15]               0\n",
      "            Conv2d-5           [-1, 32, 13, 13]           9,248\n",
      "              ReLU-6           [-1, 32, 13, 13]               0\n",
      "         MaxPool2d-7             [-1, 32, 6, 6]               0\n",
      "            Conv2d-8             [-1, 16, 4, 4]           4,624\n",
      "              ReLU-9             [-1, 16, 4, 4]               0\n",
      "      BatchNorm2d-10             [-1, 16, 4, 4]              32\n",
      "        MaxPool2d-11             [-1, 16, 2, 2]               0\n",
      "          Flatten-12                   [-1, 64]               0\n",
      "           Linear-13                    [-1, 6]             390\n",
      "          Softmax-14                    [-1, 6]               0\n",
      "================================================================\n",
      "Total params: 15,254\n",
      "Trainable params: 15,254\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.81\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.88\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinz/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "#################### NETWORK SMALL ####################\n",
    "#######################################################\n",
    "#######################################################\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "                   \n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 1, padding = 0)\n",
    "        self.relu_conv_1 = nn.ReLU()\n",
    "        self.Batch_Norm2D_1 = nn.BatchNorm2d(32)\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 0)\n",
    "        self.relu_conv_2 = nn.ReLU()\n",
    "        #self.drop2d_1 = nn.Dropout2d(p=0.2)\n",
    "        #self.Batch_Norm2D_2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 0)\n",
    "        self.relu_conv_3 = nn.ReLU()\n",
    "        self.Batch_Norm2D_3 = nn.BatchNorm2d(16)\n",
    "        self.maxpool_3 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "\n",
    "        \n",
    "        #in_channels = 10\n",
    "        #out_channels = 32\n",
    "        #self.depth_conv_1 = Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, groups=10)\n",
    "        #self.point_conv_1 = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n",
    "        \n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        #self.Linear1 = nn.LazyLinear(12)\n",
    "        #self.relu1 = nn.ReLU()\n",
    "        #self.drop1 = nn.Dropout(0.5)\n",
    "        #self.BatchNorm1 = nn.LazyBatchNorm1d()\n",
    "        \n",
    "        #self.Linear2 = nn.LazyLinear(12)\n",
    "        #self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.Linear3 = nn.LazyLinear(6)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu_conv_1(x)\n",
    "        x = self.Batch_Norm2D_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu_conv_2(x)\n",
    "        #x = self.drop2d_1(x)\n",
    "        #x = self.Batch_Norm2D_2(x)\n",
    "        x = self.maxpool_2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu_conv_3(x)\n",
    "        x = self.Batch_Norm2D_3(x)\n",
    "        x = self.maxpool_3(x)\n",
    "        \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        #x = self.Linear1(x)\n",
    "        #x = self.relu1(x)\n",
    "        #x = self.drop1(x)\n",
    "        #x = self.BatchNorm1(x)\n",
    "        \n",
    "        x = self.Linear3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.Adam(network.parameters())\n",
    "summary(network, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c89ee4-0ce4-495a-a514-394008632777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 62, 62]             896\n",
      "              ReLU-2           [-1, 32, 62, 62]               0\n",
      "       BatchNorm2d-3           [-1, 32, 62, 62]              64\n",
      "         MaxPool2d-4           [-1, 32, 31, 31]               0\n",
      "            Conv2d-5           [-1, 64, 31, 31]          18,496\n",
      "              ReLU-6           [-1, 64, 31, 31]               0\n",
      "       BatchNorm2d-7           [-1, 64, 31, 31]             128\n",
      "            Conv2d-8           [-1, 16, 31, 31]           9,232\n",
      "              ReLU-9           [-1, 16, 31, 31]               0\n",
      "           Conv2d-10           [-1, 16, 31, 31]           1,040\n",
      "             ReLU-11           [-1, 16, 31, 31]               0\n",
      "        MaxPool2d-12           [-1, 32, 15, 15]               0\n",
      "           Conv2d-13           [-1, 16, 13, 13]           4,624\n",
      "             ReLU-14           [-1, 16, 13, 13]               0\n",
      "      BatchNorm2d-15           [-1, 16, 13, 13]              32\n",
      "           Conv2d-16            [-1, 8, 11, 11]           1,160\n",
      "             ReLU-17            [-1, 8, 11, 11]               0\n",
      "          Flatten-18                  [-1, 968]               0\n",
      "           Linear-19                   [-1, 32]          31,008\n",
      "             ReLU-20                   [-1, 32]               0\n",
      "          Dropout-21                   [-1, 32]               0\n",
      "           Linear-22                    [-1, 6]             198\n",
      "          Softmax-23                    [-1, 6]               0\n",
      "================================================================\n",
      "Total params: 66,878\n",
      "Trainable params: 66,878\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 5.07\n",
      "Params size (MB): 0.26\n",
      "Estimated Total Size (MB): 5.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "#################### NETWORK LARGE ####################\n",
    "#######################################################\n",
    "#######################################################\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "                   \n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 1, padding = 0)\n",
    "        self.relu_conv_1 = nn.ReLU()\n",
    "        self.Batch_Norm2D_1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        \n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu_conv_2 = nn.ReLU()\n",
    "        self.Batch_Norm2D_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 16, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu_conv_3 = nn.ReLU()\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 16, kernel_size = 1, stride = 1, padding = 0)\n",
    "        self.relu_conv_5 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.maxpool_6 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "        self.conv6 = nn.Conv2d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 0)\n",
    "        self.relu_conv_6 = nn.ReLU()\n",
    "        self.Batch_Norm2D_6 = nn.BatchNorm2d(16)\n",
    "        \n",
    "\n",
    "        self.conv7 = nn.Conv2d(in_channels = 16, out_channels = 8, kernel_size = 3, stride = 1, padding = 0)\n",
    "        self.relu_conv_7 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.Linear1 = nn.LazyLinear(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.Linear3 = nn.LazyLinear(6)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu_conv_1(x)\n",
    "        x = self.Batch_Norm2D_1(x)\n",
    "        \n",
    "        x = self.maxpool_2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu_conv_2(x)\n",
    "        x = self.Batch_Norm2D_2(x)\n",
    "        \n",
    "        x1 = self.conv3(x)\n",
    "        x1 = self.relu_conv_3(x1)\n",
    "        \n",
    "        x2 = self.conv5(x)\n",
    "        x2 = self.relu_conv_5(x2)\n",
    "        \n",
    "        x = torch.cat((x1,x2),1)\n",
    "        \n",
    "        x = self.maxpool_6(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu_conv_6(x)\n",
    "        x = self.Batch_Norm2D_6(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.relu_conv_7(x)\n",
    "        \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.Linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        x = self.Linear3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.Adam(network.parameters())\n",
    "summary(network, input_size=(3, 64, 64))\n",
    "\n",
    "\n",
    "#### CURRENTTTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e83df700-683a-41f2-8bfc-81dc7a028201",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, label = next(iter(test_ds))\n",
    "batch_np = batch.to(device)\n",
    "yhat = network(batch_np) # Give dummy batch to forward().\n",
    "\n",
    "input_names = ['Input']\n",
    "output_names = ['Output']\n",
    "torch.onnx.export(network, batch_np, 'Network.onnx', input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7cc6c-92f0-4583-ba8a-17d5531fd5a9",
   "metadata": {},
   "source": [
    "#### tb = SummaryWriter()\n",
    "images, labels = next(iter(train_ds))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "tb.add_image(\"images\", grid)\n",
    "tb.add_graph(network, images)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc43909a-ac5f-41a3-a4b6-4a2d9a33a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader_train, dataloader_val, lambdas, criterion, optimizer, scheduler, num_epochs=25, Fine_Tuning = False):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    lambda_1, lambda_2 = lambdas\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        if Fine_Tuning and (epoch > 0.9*num_epochs):\n",
    "            for param in network.transfer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = dataloader_train\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                dataloader = dataloader_val\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloader, total = len(dataloader)):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # add l1 and l2 regularization\n",
    "                    #l1_regularization, l2_regularization = 0,0\n",
    "                    #for param in model.parameters():\n",
    "                    #    l1_regularization += lambda_1*torch.norm(param, 1)#**2\n",
    "                    #    l2_regularization += lambda_2*torch.norm(param, 2)**2\n",
    "                        \n",
    "                    loss = criterion(outputs, labels) #+ l1_regularization + l2_regularization\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "                writer.add_scalar(\"Accuracy/train\", epoch_acc, epoch)\n",
    "            else:         \n",
    "                writer.add_scalar(\"Loss/test\", epoch_loss, epoch)\n",
    "                writer.add_scalar(\"Accuracy/test\", epoch_acc, epoch)\n",
    "            \n",
    "            for name, weight in model.named_parameters():\n",
    "                tb.add_histogram(name,weight, epoch)\n",
    "                tb.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                Save_path = f\"./Pytorch_Model/checkpoints/EPOCH_{epoch}_ACC:_{epoch_acc}_%\"\n",
    "                torch.save(model.state_dict(), Save_path)\n",
    "                \n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    return model, writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f960da5a-69e6-4dcc-b5f7-536e74de64af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [02:50<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.5499 Acc: 0.5360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:30<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4625 Acc: 0.5984\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [02:05<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.4337 Acc: 0.6299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:25<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.3699 Acc: 0.6846\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [01:48<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3973 Acc: 0.6615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:25<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.3764 Acc: 0.6654\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [01:45<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3778 Acc: 0.6736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:26<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.3361 Acc: 0.7103\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [01:45<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3651 Acc: 0.6859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:27<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.3519 Acc: 0.6917\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [01:55<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3521 Acc: 0.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:26<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.3288 Acc: 0.7199\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [01:58<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3463 Acc: 0.7024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:34<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.3170 Acc: 0.7281\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [02:15<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3371 Acc: 0.7112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:53<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.3184 Acc: 0.7245\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:34<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3252 Acc: 0.7214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:38<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2968 Acc: 0.7463\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [02:23<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3273 Acc: 0.7164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:37<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2946 Acc: 0.7520\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [02:43<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3196 Acc: 0.7256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2880 Acc: 0.7555\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [02:45<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3155 Acc: 0.7271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:46<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2966 Acc: 0.7502\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:12<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3219 Acc: 0.7214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:50<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2989 Acc: 0.7459\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:35<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3148 Acc: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:52<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2900 Acc: 0.7509\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:45<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3084 Acc: 0.7361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:48<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2824 Acc: 0.7612\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:33<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3076 Acc: 0.7367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:48<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2828 Acc: 0.7591\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:05<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2929 Acc: 0.7527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:40<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2833 Acc: 0.7591\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [02:42<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3005 Acc: 0.7434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:42<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2793 Acc: 0.7591\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [02:48<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2940 Acc: 0.7490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:42<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2794 Acc: 0.7627\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [02:50<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2961 Acc: 0.7483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:31<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2776 Acc: 0.7644\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [04:58<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2953 Acc: 0.7490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:27<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2846 Acc: 0.7552\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:21<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2902 Acc: 0.7534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:26<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2846 Acc: 0.7562\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:24<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3202 Acc: 0.7222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:38<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2949 Acc: 0.7420\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:23<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2924 Acc: 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:25<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2822 Acc: 0.7591\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [04:06<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2848 Acc: 0.7588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:53<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2800 Acc: 0.7609\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [04:09<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2684 Acc: 0.7756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:52<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2580 Acc: 0.7869\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:50<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2648 Acc: 0.7795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [01:02<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2582 Acc: 0.7819\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:48<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2570 Acc: 0.7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:59<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2556 Acc: 0.7851\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [04:17<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2581 Acc: 0.7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:58<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2511 Acc: 0.7912\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:42<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2544 Acc: 0.7897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:55<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2496 Acc: 0.7944\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [03:46<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2554 Acc: 0.7895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [00:51<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2452 Acc: 0.7994\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 351/351 [04:34<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2489 Acc: 0.7956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 88/88 [01:15<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2517 Acc: 0.7904\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▊                                     | 40/351 [00:24<03:11,  1.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jp/vwgcg4z55_dc554x_d2yfzvm0000gn/T/ipykernel_70961/1709013955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mFine_Tuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m network, history = train_model(network, train_ds, val_ds, lambdas, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0m\u001b[1;32m     19\u001b[0m                            num_epochs = 100, Fine_Tuning = False)\n",
      "\u001b[0;32m/var/folders/jp/vwgcg4z55_dc554x_d2yfzvm0000gn/T/ipykernel_70961/1157060940.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader_train, dataloader_val, lambdas, criterion, optimizer, scheduler, num_epochs, Fine_Tuning)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m    229\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network = network.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(network.parameters(), lr=0.001)#, weight_decay=1e-3)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n",
    "\n",
    "lambdas = (1e-5, 1e-5)\n",
    "lambdas = (1e-5, 1e-5) # NOT USED\n",
    "\n",
    "# If Fine_Tuning = True, the whole network (including pretrained one)\n",
    "# will be trained in the last 10% of the total epochs\n",
    "Fine_Tuning = True\n",
    "\n",
    "network, history = train_model(network, train_ds, val_ds, lambdas, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                           num_epochs = 100, Fine_Tuning = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ff11c-2513-4832-aa83-7eade10b654d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29314cf-1cab-4734-9d48-b13adeb3e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataset, total = len(dataset)):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = network(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "    return 100 * correct // total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30782fba-ec8d-4ab4-8dec-e552a2fb9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = validate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b527f7-be3f-45f2-8849-cbbf60c96698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_validate(dataset):\n",
    "    classes = dataset.dataset.classes\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    \n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataset, total = len(dataset)):\n",
    "            images, labels = data\n",
    "            outputs = network(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "    \n",
    "    \n",
    "    # print accuracy for each class\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c3d96-6a70-4a41-b3b0-fd0e9bdd0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_validate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00280d-4586-48af-be2d-22fd5b0a74b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ccc0f4-9991-48e4-9b3b-b51b060e1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# set to true if want to save model\n",
    "dt_string = datetime.now().isoformat()\n",
    "\n",
    "Save_path = f\"./Pytorch_Model/ACC:_{accuracy}_%__{dt_string}\"\n",
    "torch.save(network.state_dict(), Save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2a66af7-7f92-4a05-850d-801475c879f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Model\n",
    "#model = Net()\n",
    "#model.load_state_dict(torch.load(Save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615d359-958f-44fd-bab4-1296717fe046",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b61bbc-a041-4902-8222-ebeee7d9be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class_names = train_ds.dataset.dataset.classes\n",
    "\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "def visualize_model(model, num_images=8):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(val_ds):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]} \\n actual: {class_names[labels[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323a219-ef8c-4324-be9d-a8e569b72a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_model(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063ed9d-6db6-4a98-ac1f-9912ce298041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
